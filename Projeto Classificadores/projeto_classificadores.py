# -*- coding: utf-8 -*-
"""Projeto_Classificadores.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dk0-S8d-cndh2T1ddIb3fqtZdSHmlo4s

*   Projeto Classificadores 
*   Aluno: Marcio Rafael Buzoli ST3004414

*   Disciplina: Mineração de Dados

## Importando bibliotecas
"""

import pandas as pd
import seaborn as sns
import matplotlib as plt
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn import tree
from matplotlib.figure import Figure
from sklearn.metrics import accuracy_score


df = pd.read_csv('/content/drive/MyDrive/Faculdade/Data_Mining/Dataset Teste.csv',sep=',')
dados = df

"""# Dados

"""

df.head()

"""Os dados presentes nesse dataset são referentes ao funcionamento de uma bomba centrifuga em diferentes estados de operação, os atributos são baseados nos seguintes parâmetros:

*	Corrente elétrica
*	Torque
*	Fator de potencia

Para cada parâmetro é possível extrair diversos dados, para nosso estudo levamos em consideração os 9 atributos a seguir:

*	Média
*	Desvio Padrão
*	Valor máximo
*	Valor mínimo
*	Range
*	Kurtosis
*	RMS (root mean square, raiz quadrática média)
*	Skewness (Distorção)
*	Entropia



"""

df['Class'].value_counts()

"""As leituras estão classificadas em 3 status de funcionamento:

* Classe 0 – Funcionamento Normal 
* Classe 1 – Funcionamento com cavitação 
* Classe 2 – Funcionamento a seco

"""

dados.describe().round(6)

dados.corr().round(6)

"""# Normalizando os dados"""

df_norm = df[['AVG Current','STD Current', 'MAX Current', 'MIN Current', 'Range Current', 'Kurtosis Current', 'RMS Current', 'Skewness Current', 'Entropy Current', 'AVG Torque', 'STD Torque', 'MAX Torque', 'MIN Torque', 'Range Torque', 'Kurtosis Torque', 'RMS Torque', 'Skewness Torque', 'Entropy Torque','AVG Active Power', 'STD Active Power', 'MAX Active Power', 'MIN Active Power', 'Range Active Power', 'Kurtosis Active Power', 'RMS Active Power', 'Skewness Active Power', 'Entropy Active Power','Class']].apply(lambda x: (x - x.min()) / (x.max() - x.min()))
df_norm.describe()

sns.pairplot(df_norm)

"""A partir dos graficos acima, observamos a baixa correlação entre as diversas variáveis. A seguir serão separados dados para treinamento e testes dos classificadores

# Separação d dados para Treinamento e Testes

Separação de dados
"""

y = df.iloc[:, 27]
y.values

"""# Definindo os dados para analise

"""

x_train, x_test, y_train, y_test = train_test_split(df_norm, y, test_size = 0.2)
x_test.describe()

"""# **Algoritimos para Classificação de Estrelas**

# Árvore de Decisão

Criando o Modelo
"""

classificador = tree.DecisionTreeClassifier()
arvore = classificador.fit(x_train,y_train)

"""Treinando e Testando"""

arvore = classificador.fit(x_train,y_train)

"""Comparando a resposta do algoritmo com as respostas reais do dataset"""

y_resp = arvore.predict(x_test)
y_resp

y_test[:]

plt.figure(figsize = (20,10))
tree.plot_tree(classificador,filled=True,fontsize=14)
plt.savefig('Árvore')
plt.close

classificador.get_depth()

"""Observamos que a arvore de decisão é relativamente simples, possui apenas 2 camadas, isso facilitará uma Análise utilizando esta base. """

accuracy_score(y_test,y_resp)

"""Alto valor de acurácia devido a base de dados.

# Vizinho mais próximo (KNN)
"""

from sklearn.neighbors import KNeighborsClassifier

"""A seguir será padronizado a classificação pelos 10 vizinhos mais próximos """

kclassifier = KNeighborsClassifier(n_neighbors=10)

"""Treinando e Testanto"""

kclassifier.fit(x_train,y_train)

y_resp=kclassifier.predict(x_test)
y_resp

y_test

from sklearn.metrics import accuracy_score
accuracy_score(y_test,y_resp)

"""Observa-se que a acurácia é igual algoritmo anterior."""

from sklearn.metrics import classification_report

print(classification_report(y_test,y_resp,target_names = ["0","1","2"]))

"""Nas linhas a cima analisamos as respostas utilizando a ferramenta de Classification_Report

# Naive Bayes
"""

from sklearn.naive_bayes import GaussianNB

"""Criando modelo """

naiveBayesModel = GaussianNB()

"""Treinando """

naiveBayesModel.fit(x_train,y_train)

y_resp=naiveBayesModel.predict(x_test)
y_resp

y_test

accuracy_score(y_test,y_resp)

"""Pela acurácia percebemos que o Naive Bayes não é tão preciso quanto a arvore de decisão ou KNN, mas apresenta um resultado satisfatório. A seguir podemos observamos informações de nossa classificação de forma mais detalhada. """

from sklearn.metrics import classification_report

print(classification_report(y_test,y_resp,target_names = ["0","1","2"]))

"""# SVM

Importando bibliotecas
"""

from sklearn import svm 
from sklearn.model_selection import train_test_split

"""Com o intuíto de melhorar os resultados com o algoritmo SVM, a base de dados foi separada novamente afim de aumentar o range de treinamentos"""

from google.colab import drive
drive.mount('/content/drive')

X_train, X_test, Y_train, Y_teste = train_test_split(x_test,y_test,test_size=0.2, random_state=10)

"""Treinando e Testando"""

svmClassifier= svm.SVC()

svmClassifier.fit(X_train,Y_train)

y_resp = svmClassifier.predict(X_test)  
y_resp

accuracy_score(Y_teste,y_resp)

"""# **Conclusão**

Neste trabalho confeccionei um Dataset com dados reais das capturas de pacotes de minha iniciação cientifica, alguns atributos possuíam dados determinantes entre as entradas e a saída, isto ocasionou uma acurácia alta em todos os modelos, o Algoritmo que mais se destacou foi a Árvore de decisão, KNN e SVM (todos com acurácia de 100%), %), Naive Bayes vem na sequencia com acurácia de 97%. 
Foram utilizados os mesmos dados para treinamento objetivando comparar cada classificador, como muitos apresentaram resultados positivos, acredito precisaremos reavaliar a base de dados para novos testes.
"""